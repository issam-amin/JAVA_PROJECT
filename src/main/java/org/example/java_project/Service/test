/*
package org.example.java_project.Service;

import javafx.concurrent.Task;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.HashMap;

public class RectypeJob extends Task<HashMap<String, Integer>> {
    private final String input;
    private final String output;
    private final String jobName;
    private final String currentDate;
    private final JobType JobType;

    public RectypeJob(String jobName, JobType JobType) {
        this.JobType = JobType;
        this.jobName = jobName;
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd");
        this.currentDate = LocalDate.now().format(formatter);
        this.input = "/test/input/data.csv";
        this.output = "/test/output_" + currentDate + "_" + jobName;
    }

    @Override
    public HashMap<String, Integer> call() throws IOException {
        boolean outputExists = checkIfOutputExists(output);

        switch (JobType) {
            case NORMAL:
                if (outputExists) {
                    System.out.println("Output already exists. Reading existing results...");
                    return formatReturn();
                } else {
                    return runHadoopJob();
                }
            case REFRESH:
                return runHadoopJob();
        }
        return null;
    }

    private boolean checkIfOutputExists(String outputPath) throws IOException {
        // Use 'wsl' to check if the HDFS path exists
        ProcessBuilder pb = new ProcessBuilder("wsl", "/usr/local/hadoop/bin/hadoop", "fs", "-ls", outputPath);

        Process process = pb.start();
        try {
            return process.waitFor() == 0; // If the command succeeds, the path exists
        } catch (InterruptedException e) {
            throw new RuntimeException("Error checking if output exists", e);
        }
    }

    private HashMap<String, Integer> runHadoopJob() throws IOException {
        try {
            // Remove existing input/output if needed
            executeHadoopCommand("fs", "-rm", "-r", "-skipTrash", input);
            executeHadoopCommand("fs", "-rm", "-r", "-skipTrash", output);

            // Upload the local file to HDFS
            executeHadoopCommand("fs", "-copyFromLocal", "src/main/resources/data.csv", input);

            // Run the Hadoop MapReduce job
            String hadoopCommand = String.format(
                    "/usr/local/hadoop/bin/hadoop jar /mnt/c/path/to/your-jar-file.jar org.example.Main %s %s",
                    input, output
            );
            executeShellCommand(hadoopCommand);

            // Read the output
            return formatReturn();
        } catch (Exception e) {
            throw new RuntimeException("Error running Hadoop job", e);
        }
    }

    private void executeHadoopCommand(String... args) throws IOException, InterruptedException {
        // Combine 'wsl' and Hadoop binary path with the passed arguments
        String[] command = new String[args.length + 2];
        command[0] = "wsl";  // Use WSL
        command[1] = "/usr/local/hadoop/bin/hadoop";  // Full path to hadoop executable
        System.arraycopy(args, 0, command, 2, args.length);  // Copy the passed arguments into the array

        ProcessBuilder pb = new ProcessBuilder(command);
        Process process = pb.start();
        if (process.waitFor() != 0) {
            try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getErrorStream()))) {
                throw new RuntimeException("Hadoop command failed: " + reader.readLine());
            }
        }
    }

    private void executeShellCommand(String command) throws IOException, InterruptedException {
        // Run a shell command through WSL
        ProcessBuilder pb = new ProcessBuilder("wsl", "bash", "-c", command);
        Process process = pb.start();
        if (process.waitFor() != 0) {
            try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getErrorStream()))) {
                throw new RuntimeException("Command failed: " + reader.readLine());
            }
        }
    }

    private HashMap<String, Integer> formatReturn() throws IOException {
        // Read output from the MapReduce job
        String outputFilePath = output + "/part-r-00000";
        ProcessBuilder pb = new ProcessBuilder("wsl", "/usr/local/hadoop/bin/hadoop", "fs", "-cat", outputFilePath);
        Process process = pb.start();

        HashMap<String, Integer> statistics = new HashMap<>();
        try (BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()))) {
            String line;
            while ((line = reader.readLine()) != null) {
                String[] parts = line.split("\\t");
                statistics.put(parts[0], Integer.parseInt(parts[1]));
            }
        }

        try {
            if (process.waitFor() != 0) {
                throw new RuntimeException("Failed to read output file.");
            }
        } catch (InterruptedException e) {
            throw new RuntimeException("Error reading output file", e);
        }

        return statistics;
    }
}
*/
